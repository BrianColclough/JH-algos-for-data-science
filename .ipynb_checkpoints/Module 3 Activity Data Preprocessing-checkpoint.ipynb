{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 align=\"center\">__Module 3 Activity__</h3>\n",
    "# <h3 align=\"center\">__Assigned at the start of Module 3__</h3>\n",
    "# <h3 align=\"center\">__Due at the end of Module 3__</h3><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Discussion Forum Participation\n",
    "\n",
    "Each week, you are required to participate in the module’s discussion forum. The discussion forum consists of the week's Module Activity, which is released at the beginning of the module. You must complete/attempt the activity before you can post about the activity and anything that relates to the topic. \n",
    "\n",
    "## Grading of the Discussion\n",
    "\n",
    "### 1. Initial Post:\n",
    "Create your thread by **Day 5 (Saturday night at midnight, PST).**\n",
    "\n",
    "### 2. Responses:\n",
    "Respond to at least two other posts by **Day 7 (Monday night at midnight, PST).**\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Criteria:\n",
    "\n",
    "Your participation will be graded as follows:\n",
    "\n",
    "### Full Credit (100 points):\n",
    "- Submit your initial post by **Day 5.**\n",
    "- Respond to at least two other posts by **Day 7.**\n",
    "\n",
    "### Half Credit (50 points):\n",
    "- If your initial post is late but you respond to two other posts.\n",
    "- If your initial post is on time but you fail to respond to at least two other posts.\n",
    "\n",
    "### No Credit (0 points):\n",
    "- If both your initial post and responses are late.\n",
    "- If you fail to submit an initial post and do not respond to any others.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes:\n",
    "\n",
    "- **Late Initial Posts:** Late posts will automatically receive half credit if two responses are completed on time.\n",
    "- **Substance Matters:** Responses must be thoughtful and constructive. Comments like “Great post!” or “I agree!” without further explanation will not earn credit.\n",
    "- **Balance Participation:** Aim to engage with threads that have fewer or no responses to ensure a balanced discussion.\n",
    "\n",
    "---\n",
    "\n",
    "## Avoid:\n",
    "- A number of posts within a very short time-frame, especially immediately prior to the posting deadline.\n",
    "- Posts that complement another post, and then consist of a summary of that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Activity: Building a Preprocessing Pipeline\n",
    "\n",
    "## Objective\n",
    "Learn how to build a preprocessing pipeline in scikit-learn and apply it to the famous Iris dataset. Gain hands-on experience in handling missing values, scaling features, and understanding the importance of preprocessing pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Sample Code for Pipeline Syntax\n",
    "Here’s an example to help you understand how to create a pipeline. This pipeline imputes missing values using the mean:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Feature1    2.0\n",
      "Feature2    2.5\n",
      "dtype: float64\n",
      "Original Data:\n",
      "   Feature1  Feature2\n",
      "0       1.0       NaN\n",
      "1       NaN       2.0\n",
      "2       3.0       3.0\n",
      "\n",
      "Processed Data:\n",
      "[[1.  2.5]\n",
      " [2.  2. ]\n",
      " [3.  3. ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example dataset with missing values\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': [1.0, np.nan, 3.0],\n",
    "    'Feature2': [np.nan, 2.0, 3.0]\n",
    "})\n",
    "\n",
    "# Define a pipeline with an imputer\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "processed_data = pipeline.fit_transform(data)\n",
    "\n",
    "print(\"mean\", data.mean())\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nProcessed Data:\")\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Instructions\n",
    "\n",
    "## Dataset Preparation\n",
    "We will use the Iris dataset, randomly remove values to simulate missing data, and keep it in a Pandas DataFrame for you to preprocess.\n",
    "\n",
    "---\n",
    "\n",
    "## Your Task\n",
    "Build a preprocessing pipeline that:\n",
    "- Imputes missing values using the median.\n",
    "- Scales features to a `[0, 1]` range using `MinMaxScaler`.\n",
    "- Add at least one more preprocessing step.\n",
    "\n",
    "### Reflection\n",
    "At the end of the activity, answer the following questions:\n",
    "1. What challenges did you face while handling missing data?\n",
    "2. Why is it important to use a pipeline for preprocessing?\n",
    "---\n",
    "\n",
    "## Dataset Setup\n",
    "Run the following code to import the Iris dataset and simulate missing data. You will use this dataset for the activity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with Missing Values:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                NaN               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                NaN               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "5                5.4               3.9                1.7               0.4\n",
      "6                NaN               3.4                1.4               0.3\n",
      "7                5.0               NaN                NaN               0.2\n",
      "8                4.4               2.9                1.4               0.2\n",
      "9                4.9               3.1                1.5               0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn.preprocessing as ppr\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Randomly introduce missing values in random cells\n",
    "np.random.seed(42)\n",
    "total_cells = data.size\n",
    "num_missing = int(0.1 * total_cells)  # 10% of total cells\n",
    "missing_indices = [(row, col) for row in range(data.shape[0]) for col in range(data.shape[1])]\n",
    "random_missing_indices = np.random.choice(len(missing_indices), size=num_missing, replace=False)\n",
    "\n",
    "for index in random_missing_indices:\n",
    "    row, col = missing_indices[index]\n",
    "    data.iat[row, col] = np.nan\n",
    "\n",
    "print(\"Dataset with Missing Values:\")\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Build your pipeline** to preprocess the dataset.\n",
    "2. **Test your pipeline** by fitting it to the Iris dataset and transforming it.\n",
    "3. **Review the processed data** and reflect on how the pipeline simplifies your workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipline processed data\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0            0.222222          0.743421           0.576271          0.041667\n",
      "1            0.166667          0.480263           0.067797          0.041667\n",
      "2            0.111111          0.585526           0.576271          0.041667\n",
      "3            0.083333          0.532895           0.084746          0.041667\n",
      "4            0.194444          0.796053           0.067797          0.041667\n",
      "5            0.305556          0.953947           0.118644          0.125000\n",
      "6            0.416667          0.690789           0.067797          0.083333\n",
      "7            0.194444          0.480263           0.576271          0.041667\n",
      "8            0.027778          0.427632           0.067797          0.041667\n",
      "9            0.166667          0.532895           0.084746          0.000000\n",
      "10           0.305556          0.848684           0.084746          0.041667\n",
      "11           0.138889          0.690789           0.101695          0.041667\n",
      "12           0.138889          0.480263           0.067797          0.000000\n",
      "13           0.000000          0.480263           0.016949          0.500000\n",
      "14           0.416667          1.000000           0.033898          0.041667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# Creation of the impurter to handle the missing values\n",
    "median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "## Run our data through the pipeline to populate the missing values\n",
    "processed_data = median_imputer.fit_transform(data)\n",
    "scaler = ppr.MinMaxScaler((0,1))\n",
    "scaled_data = scaler.fit_transform(processed_data)\n",
    "\n",
    "scaled_data_frame = pd.DataFrame(scaled_data, columns=iris.feature_names)\n",
    "# print(scaled_data_frame.head(10))\n",
    "\n",
    "\"\"\"\n",
    "now that we have seen how the data should behave in a non-pipline environemnt\n",
    "lets build a pipline to standarize the process and make it reusable\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "I asked claude to help me figure out what another good preprocessing step would be\n",
    "and it suggested capping the outliers. So this function caps the outliers using the IQR method\n",
    "whichs uses the 25th and 75th percentiles to calculate the interquartile range (IQR) and then caps the outliers at 1.5 * IQR below the 25th percentile and 1.5 * IQR above the 75th percentile.\n",
    "\"\"\"\n",
    "def cap_outliers_iqr(X):\n",
    "    \"\"\"Cap outliers using IQR method\"\"\"\n",
    "    X_copy = X.copy()\n",
    "    Q1 = np.percentile(X_copy, 25, axis=0)\n",
    "    Q3 = np.percentile(X_copy, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap outliers\n",
    "    X_copy = np.clip(X_copy, lower_bound, upper_bound)\n",
    "    return X_copy\n",
    "\n",
    "outlier_capper = FunctionTransformer(cap_outliers_iqr)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', median_imputer),\n",
    "    ('outlier_capper', FunctionTransformer(cap_outliers_iqr)),\n",
    "    ('scaler', ppr.MinMaxScaler((0,1)))\n",
    "])\n",
    "\n",
    "processed_data = pipeline.fit_transform(data)\n",
    "processed_data_frame = pd.DataFrame(processed_data, columns=iris.feature_names)\n",
    "print(\"Pipline processed data\")\n",
    "print(processed_data_frame.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection on the benefits of pipelines\n",
    "\n",
    "One of the main benefits of using a pipeline is that it encapsulates every preprocessing step into a single, reusable workflow, making sure that your data always receives the same sequence of transformations and avoiding subtle mistakes that could have from re-implenting the pipeline. At the start of my script I had manually chained operations—imputing missing values and scaling values—which was error-prone and hard to reproduce. By switching to scikit-learn’s `Pipeline` class, I can bundle all of those steps (e.g. `SimpleImputer`, `OneHotEncoder`, `SandardScaler`) into one object. This not only simplifies my code, since I call `pipeline.fit_transform()` once instead of repeating each function, but also keeps my preprocessing logic in one place, and guarantees that training, validation, and test sets are handled identically every time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en-685-621",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
